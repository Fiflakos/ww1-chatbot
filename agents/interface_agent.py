# agents/interface_agent.py

import os
import streamlit as st
import requests

# â”€â”€â”€ 1) Hugging Face credentials & model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HF_API_TOKEN = os.getenv("HF_API_TOKEN")
HF_MODEL     = "google/flan-t5-base"
HF_URL       = f"https://api-inference.huggingface.co/models/{HF_MODEL}"

# â”€â”€â”€ 2) Build & cache your retriever once â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_agent():
    from retrieval_agent import RetrievalAgent
    return RetrievalAgent(corpus_dir="data_cleaned")

agent = load_agent()

# â”€â”€â”€ 3) Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="WW1 Historical Chatbot")
st.title("ğŸª– WW1 Historical Chatbot")
st.write("Ask anything across the full WW1 corpusâ€”no uploads needed.")

q = st.text_input("ğŸ’¬ What would you like to ask?")
if q:
    # â”€â”€â”€ 3a) Retrieve topâ€k passages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    hits = agent.search(q, top_k=5)
    st.write("ğŸ“Œ Top passages:")
    for i, (fn, score, snippet) in enumerate(hits, start=1):
        st.write(f"{i}. **{fn}**  (score {score:.2f})")
        st.write(snippet + "â€¦")

    # â”€â”€â”€ 4) Build your generation prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    context = "\n\n".join(f"{fn}: {snippet}" for fn, _, snippet in hits)
    prompt  = f"Context:\n{context}\n\nQuestion: {q}"

    # â”€â”€â”€ 5) Call HF Inference API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    headers = {"Authorization": f"Bearer {HF_API_TOKEN}"}
    payload = {
        "inputs": prompt,
        "parameters": {
            "max_new_tokens": 256,
            "temperature":      0.7,
            "top_p":            0.9
        },
        "options": {"wait_for_model": True}
    }

    with st.spinner("ğŸ§  Thinking..."):
        resp = requests.post(HF_URL, headers=headers, json=payload)
        if resp.status_code == 200:
            data = resp.json()
            # HF /models endpoint returns either a list or dict
            if isinstance(data, list) and data and "generated_text" in data[0]:
                answer = data[0]["generated_text"]
            elif isinstance(data, dict) and "generated_text" in data:
                answer = data["generated_text"]
            else:
                answer = str(data)
            st.markdown(f"**ğŸ“œ Answer:** {answer}")
        else:
            st.error(f"âŒ HF inference error {resp.status_code}: {resp.text}")

else:
    st.info("Enter a question to get started.")
